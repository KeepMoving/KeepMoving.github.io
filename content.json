{"meta":{"title":"互联网实践者","subtitle":"阅读使人充实，写作使人精确","description":null,"author":"Wayne Wang","url":"http://yoursite.com"},"pages":[{"title":"my first page","date":"2019-02-09T08:53:55.000Z","updated":"2019-02-09T08:53:55.524Z","comments":true,"path":"my-first-page/index.html","permalink":"http://yoursite.com/my-first-page/index.html","excerpt":"","text":""}],"posts":[{"title":"whistle使用心得","slug":"whistle使用心得","date":"2020-06-27T05:35:08.000Z","updated":"2020-06-27T05:35:34.201Z","comments":true,"path":"2020/06/27/whistle使用心得/","link":"","permalink":"http://yoursite.com/2020/06/27/whistle使用心得/","excerpt":"","text":"因为postman和fiddler这类http请求debug软件涉A，在公司内部被禁止使用。所以无奈只能在网上寻找替代品。发现很多人推荐whistle，所以就上手试了一下。 先说感受，单就配置来说，和fiddler以及postman有较大的不同。但是参考了官方文档后，发现whistle配置非常灵活，基本已经覆盖日常使用场景。其次，whistle是基于nodejs的，所以本地要安装node。再次，whistle在安装完毕后，在命令行执行w2 help可以查看所以操作。执行w2 start启动whistle服务，默认端口是8899。然后，chrome安装SwitchOmega插件，把请求转发到本地的8899端口上。最后，浏览器地址栏输入http://127.0.0.1:8899/打开whistle界面。 接下来看功能界面，左侧导航栏有network,rules,values以及plugins这四项。 network主要是用于发送请求，并配置header以及requestbody，可以点击菜单条上的compose按钮进行编辑。编辑区分布和fiddler差不多。 rules用于配置映射规则，可以按照规则给http请求添加额外的header以及cookie。我的理解类似于JAVA的AOP。 values用于存储whistle的全局变量，可以在rules中引用该变量。 官方文档参考：https://github.com/avwo/whistle/wiki 未完待续","categories":[],"tags":[]},{"title":"async-profiler+FlameGraph火焰图进行性能测试心得","slug":"async-profiler-FlameGraph火焰图进行性能测试心得","date":"2020-06-27T05:32:59.000Z","updated":"2020-06-27T05:34:01.620Z","comments":true,"path":"2020/06/27/async-profiler-FlameGraph火焰图进行性能测试心得/","link":"","permalink":"http://yoursite.com/2020/06/27/async-profiler-FlameGraph火焰图进行性能测试心得/","excerpt":"","text":"火焰图FlameGraph是用来统计各个调用栈执行时间，以此发现性能瓶颈的一种工具。火焰图的原理是通过分析一段时间内cpu执行各个方法栈的时间，得出各个方法栈执行时间比例并通过图形化的方式显示出来（其底层原理是通过linux的perf命令得到方法的调用时间比例）。因为最后生成的图片形似火焰而得名。火焰图是一个分层结构的图片，下层是调用栈，而其上一层是该方法栈所调用的各个子方法栈所执行的时间。直观的讲，如果火焰图中存在平层的峰顶，表示cpu在这段时间内一直执行该方法，也就可能存在性能的瓶颈。生成火焰图的命令包括两步：一是使用async-profiler记录方法栈执行情况。命令如下：nohup ./profiler.sh -d 300 -o collapsed -f /tmp/flame/collapsed.txt 12344 &amp;-d 300表示记录300秒内的执行情况-f /tmp/flame/collapsed.txt表示输出数据到指定文件中12344是进程号 二是基于记录的数据，生成svg图片。命令如下：./flamegraph.pl –colors=java /tmp/flame/collapsed.txt &gt; collapsed.svg表示读取/tmp/flame/collapsed.txt的数据生成名为collapsed.svg的图片文件。该图片文件可用chrome打开，速度还是比较快的。（顺便吐槽一下IE，打开中途直接卡死了）","categories":[],"tags":[]},{"title":"注册systemv服务并限制资源","slug":"注册systemv服务并限制资源","date":"2020-06-27T05:31:09.000Z","updated":"2020-06-27T05:31:58.249Z","comments":true,"path":"2020/06/27/注册systemv服务并限制资源/","link":"","permalink":"http://yoursite.com/2020/06/27/注册systemv服务并限制资源/","excerpt":"","text":"1.新增systemv服务文件/etc/init.d/xxx服务2.新增/etc/pam.d/xxx服务，里面设置session required pam_limits.so3.修改/etc/security/limits.conf文件新增一行* hard nproc 10240，限制每个用户的最大进程数4.新开一个窗口用service xxx服务 stop和start重启xxx服务","categories":[],"tags":[]},{"title":"dubbo网络通信的编解码过程解析","slug":"dubbo网络通信的编解码过程解析","date":"2019-03-24T11:04:33.000Z","updated":"2019-03-30T04:36:20.835Z","comments":true,"path":"2019/03/24/dubbo网络通信的编解码过程解析/","link":"","permalink":"http://yoursite.com/2019/03/24/dubbo网络通信的编解码过程解析/","excerpt":"编码和解码首先做一下名词解释编码：序列化，它将对象序列化为字节数组，用于网络传输，数据持久化或者其他用途。解码：反序列化，把从网络，磁盘等读取的字节数还原成原始对象，以方便后续的业务逻辑操作。 dubbo在IP和TCP协议之上，又封装了自己的一层协议，为了解决TCP出现的粘包和拆包问题。解决粘包和拆包问题有几个公认的方法： 消息定长，例如固定为1000个字节 在包尾增加回车或空格等特殊字符作为切割。典型的有FTP协议。 将消息分为消息头和消息体。例如dubbo (dubbo的消息头是定长的16个字节)。 dubbo网络通信的编解码过程分为几个步骤： consumer请求编码 provider请求解码 provider的响应结果编码(如果有返回值的话) consumer的响应结果解码（如果有返回值的话）","text":"编码和解码首先做一下名词解释编码：序列化，它将对象序列化为字节数组，用于网络传输，数据持久化或者其他用途。解码：反序列化，把从网络，磁盘等读取的字节数还原成原始对象，以方便后续的业务逻辑操作。 dubbo在IP和TCP协议之上，又封装了自己的一层协议，为了解决TCP出现的粘包和拆包问题。解决粘包和拆包问题有几个公认的方法： 消息定长，例如固定为1000个字节 在包尾增加回车或空格等特殊字符作为切割。典型的有FTP协议。 将消息分为消息头和消息体。例如dubbo (dubbo的消息头是定长的16个字节)。 dubbo网络通信的编解码过程分为几个步骤： consumer请求编码 provider请求解码 provider的响应结果编码(如果有返回值的话) consumer的响应结果解码（如果有返回值的话） consumer请求编码首先说一下结论，然后对照代码解释一下 consumer请求编码调用栈–&gt;NettyCodecAdapter.InternalEncoder.encode –&gt;DubboCountCodec.encode –&gt;ExchangeCodec.encode –&gt;ExchangeCodec.encodeRequest –&gt;DubboCodec.encodeRequestData第1-2个字节：一个魔数数字（就是一个固定的数字）第3个字节：双向或单向的标记。双向是指请求有去有回，有返回值。单向是指请求有去无回，没有返回值。第4个字节：在request请求中空着第5-12个字节：请求id,long型的8个字节。异步变同步的全局唯一ID，用来做Consumer和Provider的来回通信标记。第13-16个字节：消息体长度，也就是消息头+请求数据的长度。 以下是编码的入口函数然后判断编码的类型是针对请求的还是请求响应的然后是编码的主要方法第214行是选择序列化组件第215行HEADER_LENGTH==16，表示消息头是一个16个字节的数组。第218行是在头2个字节中写入魔数，MAGIC是一个固定的数字第221行到228行是在第3个字节处设置双向或者单向标记231行，在第5个字节开始写入请求id，该id也用作请求返回的唯一标识251行，从第13个字节开始写入消息体长度最后就是写入消息 provider的请求解码–&gt;NettyCodecAdapter.InternalDecoder.messageReceived –&gt;DubboCountCodec.decode –&gt;ExchangeCodec.decode –&gt;ExchangeCodec.decodeBody请求解码的字段意义就是上一步请求编码的字段意义。以下是请求解码的关键方法：可以看到这里是一个while循环，主要是为了解决TCP协议拆包的问题，如果没有达到指定的包长度，就循环的接收。如果出现粘包的问题，则根据消息体长度截断。这里是ExchangeCodec.decode调用，主要是截取消息头。这里是解析的关键方法。可以看到，首先校验一下头两个字节的魔数是否正确，如果不正确就直接返回。然后判断头部长度是否是16，如果不是，也是返回异常结果。然后再取出消息体长度，根据消息体长度来解包解码内容过程先判断单向请求还是双向请求，然后分别处理以双向请求为例，通过解码得到数据data的对象，该对象包括了rpc的方法名和参数以及dubbo版本号等一系列数据至此，请求解码过程结束 provider的响应结果编码–&gt; NettyCodecAdapter.InternalEncoder.encode –&gt;DubboCountCodec.encode –&gt;ExchangeCodec.encode –&gt;ExchangeCodec.encodeResponse –&gt;DubboCodec.encodeResponseData 其中DubboCodec.encodeResponseData是先写入一个字节。这个字节可能是RESPONSE_VALUE或者RESPONSE_NULL_VALUE或者RESPONSE_WITH_EXCEPTION响应结果编码的消息头还是一个16字节定长的数组第1-2个字节：魔数数字（固定数字）第3个字节：序列化组件类型，它用于和客户端约定序列化编码号第4个字节：response的结果响应码，例如OK=20第5-12个字节：请求id第13-16个字节：消息体长度 因为请求以及结果响应的encode都是一套入口，所以首先还是判断encode类型是request还是response 然后按照字段意义设置byte数组，基本和请求的encode类似，但是需要注意的是第三个字节设置的是序列化组件的类型（比如hessian在dubbo中定义的类型是2）,以及第4个字节是response的结果响应码，比如OK==20 在最后的encodeResponseData方法中会根据返回结果，塞入RESPONSE_VALUE或者RESPONSE_NULL_VALUE或者RESPONSE_WITH_EXCEPTION的响应码字节。最后是放入attachments参数。 consumer的响应结果解码consumer的响应结果解码和之前request的响应结果解码类似，就不赘述了。需要注意的是响应结果的第三个字节是序列化组件类型，第4个字节是response的结果响应码–&gt; NettyCodecAdapter.InternalDecoder.messageReceived –&gt;DubboCountCodec.decode –&gt;ExchangeCodec.decode –&gt;DubboCodec.decodeBody –&gt;DecodeableRPCResult.decode其中DecodeableRPCResult.decode根据RESPONSE_VALUE或者RESPONSE_NULL_VALUE或者RESPONSE_WITH_EXCEPTION的响应码字节做相应的处理。","categories":[],"tags":[]},{"title":"定位springboot启动异常","slug":"定位springboot启动异常","date":"2019-02-27T13:40:54.000Z","updated":"2019-03-02T12:31:44.318Z","comments":true,"path":"2019/02/27/定位springboot启动异常/","link":"","permalink":"http://yoursite.com/2019/02/27/定位springboot启动异常/","excerpt":"最近在springboot中集成kafka，然后启动的时候抛出异常该如何定位问题？基本思路是用远程debug（也可以在本地启动）+ Java Exception Breakpoints","text":"最近在springboot中集成kafka，然后启动的时候抛出异常该如何定位问题？基本思路是用远程debug（也可以在本地启动）+ Java Exception Breakpoints上图中，抛出了TypeNotPresentExceptionProxy的异常，把断点打到TypeNotPresentExceptionProxy的构造函数，可以截获到如下信息：看的出来是因为PropertyPlaceholderAutoConfiguration这个类的缺失。知道了异常的原因，下一步就是定位在哪里抛出异常，通过Java Exception Breakpoints。在java的debug视图中，添加Java Exception Breakpoints，指定具体异常是ArrayStoreException,如图:然后启动springboot工程，等抛出ArrayStoreException异常前，就会定位到具体的代码，如下图从异常信息可以看到发生异常的类是KafkaBinderConfiguration可以看到KafkaBinderConfiguration引用的是springboot1.x版本的PropertyPlaceholderAutoConfiguration，包路径是org.springframework.boot.autoconfigure.PropertyPlaceholderAutoConfiguration，而我们项目用的springboot是2.x，这个类是在org.springframework.boot.autoconfigure.context下面。因为springboot在升级2.x版本后，改动了一些包路径，所以导致启动异常","categories":[],"tags":[]},{"title":"自己动手搭建kafka及源码阅读环境","slug":"自己动手搭建kafka及源码阅读环境","date":"2019-02-24T06:55:44.000Z","updated":"2019-03-04T15:36:49.669Z","comments":true,"path":"2019/02/24/自己动手搭建kafka及源码阅读环境/","link":"","permalink":"http://yoursite.com/2019/02/24/自己动手搭建kafka及源码阅读环境/","excerpt":"搭建kafka源码阅读环境1、配置本地的jdk环境及IDE，jdk最好是1.8以上2、下载并配置gradle下载地址 http://services.gradle.org/distributions/选择gradle版本的时候，要注意配合你所下载的kafka版本我所在项目使用的kafka版本是0.9.0.1，然后我下载了0.9.x版本的kafka源码，同时下载最新版的gradle，但是本地构建的时候，一直报如下错误以及如下的错误1234Details: groovy.lang.MissingPropertyException: Could not get unknown property &apos;classesDir&apos; for main classes of type org.gradle.api.internal.tasks.DefaultSourceSetOutput.&lt;/i&gt;Warning:&lt;i&gt;&lt;b&gt;root project &apos;kafka-0.9.0.1-src&apos;: Unable to resolve additional project configuration.&lt;/b&gt;Details: groovy.lang.MissingPropertyException: Could not get unknown property &apos;projectConfiguration&apos; for DefaultProjectDependency&#123;dependencyProject=&apos;project &apos;:clients&apos;&apos;, configuration=&apos;default&apos;&#125; of type org.gradle.api.internal.artifacts.dependencies.DefaultProjectDependency.&lt;/i&gt; 这都是因为gradle和kafka源码版本不兼容。为了确定哪个版本的gradle和kafka版本兼容，我在kafka源码中全局搜了一下gradle然后我猜测0.9.0.1版本的源码需要配合2.2.1版本的gradle，安装相应的grade证实，确实是这样的安装过程很简单，下载gradle，解压到本地目录，设置系统环境GRADLE_HOME：D:\\Program Files\\gradle-2.2.1PATH：%GRADLE_HOME%\\bin在IDE中设置gradle环境Gradle home: D:/Program Files/gradle-2.2.1JDK选择1.8 3、安装scala插件因为kafka部分源码是用scala编写的，比如Consumer类的父类ShutdownableThread，所以IDE需要继承scala插件，否则会报编译错误具体步骤是在settings-&gt;Plugins-&gt;Browse Repositories中搜索scala插件，点击安装即可 4、导入kafka源码点击import project，选择kafka源码目录，然后选择gradle项目导入导入后，打卡ide的gradle视图，选择refresh项目，即可","text":"搭建kafka源码阅读环境1、配置本地的jdk环境及IDE，jdk最好是1.8以上2、下载并配置gradle下载地址 http://services.gradle.org/distributions/选择gradle版本的时候，要注意配合你所下载的kafka版本我所在项目使用的kafka版本是0.9.0.1，然后我下载了0.9.x版本的kafka源码，同时下载最新版的gradle，但是本地构建的时候，一直报如下错误以及如下的错误1234Details: groovy.lang.MissingPropertyException: Could not get unknown property &apos;classesDir&apos; for main classes of type org.gradle.api.internal.tasks.DefaultSourceSetOutput.&lt;/i&gt;Warning:&lt;i&gt;&lt;b&gt;root project &apos;kafka-0.9.0.1-src&apos;: Unable to resolve additional project configuration.&lt;/b&gt;Details: groovy.lang.MissingPropertyException: Could not get unknown property &apos;projectConfiguration&apos; for DefaultProjectDependency&#123;dependencyProject=&apos;project &apos;:clients&apos;&apos;, configuration=&apos;default&apos;&#125; of type org.gradle.api.internal.artifacts.dependencies.DefaultProjectDependency.&lt;/i&gt; 这都是因为gradle和kafka源码版本不兼容。为了确定哪个版本的gradle和kafka版本兼容，我在kafka源码中全局搜了一下gradle然后我猜测0.9.0.1版本的源码需要配合2.2.1版本的gradle，安装相应的grade证实，确实是这样的安装过程很简单，下载gradle，解压到本地目录，设置系统环境GRADLE_HOME：D:\\Program Files\\gradle-2.2.1PATH：%GRADLE_HOME%\\bin在IDE中设置gradle环境Gradle home: D:/Program Files/gradle-2.2.1JDK选择1.8 3、安装scala插件因为kafka部分源码是用scala编写的，比如Consumer类的父类ShutdownableThread，所以IDE需要继承scala插件，否则会报编译错误具体步骤是在settings-&gt;Plugins-&gt;Browse Repositories中搜索scala插件，点击安装即可 4、导入kafka源码点击import project，选择kafka源码目录，然后选择gradle项目导入导入后，打卡ide的gradle视图，选择refresh项目，即可 部署kafka从kafka官网上选择一个版本官网地址:http://kafka.apache.org/downloads 下载kafka运行包 curl -L -O https://archive.apache.org/dist/kafka/0.9.0.1/kafka_2.11-0.9.0.1.tgz 解压 tar zxvf kafka_2.11-0.9.0.1.tgz 进入config/server.properties，主要配置三个参数：broker.id、log.dir、zookeeper.connect 启动kafka，执行bin/kafka-server-start.sh config/server.properties &amp; 如果报以下错误 是因为jdk版本不兼容，要求jdk版本在1.7以上 可以在bin/kafka-run-class.sh中指定jdk版本，如图 进入kafka目录，执行bin/kafka-server-start.sh config/server.properties &amp;如果启动报如下错误，是因为config/server.properties中的broker.id重复，重新指定一个即可","categories":[],"tags":[]},{"title":"spring的事务切面（下）","slug":"spring的事务切面（下）","date":"2019-02-23T06:29:48.000Z","updated":"2019-03-02T12:42:20.374Z","comments":true,"path":"2019/02/23/spring的事务切面（下）/","link":"","permalink":"http://yoursite.com/2019/02/23/spring的事务切面（下）/","excerpt":"事务处理拦截器的配置和创建过程建立事务处理对象的时序图","text":"事务处理拦截器的配置和创建过程建立事务处理对象的时序图Spring为声明式事务处理的实现所作的一些准备工作：包括为AOP配置基础设施，这些基础设施包括设置拦截器TransactionInterceptor、通知器DefaultPointcutAdvisor或TransactionAttributeSourceAdvisor。同时，在TransactionProxyFactoryBean的实现中，还可以看到注入进来的PlatformTransactionManager和事务处理属性TransactionAttribute等。这个拦截器TransactionInterceptor通过AOP发挥作用，通过这个拦截器的实现，Spring封装了事务处理实现。依赖注入的PlatformTransactionManager。通过依赖注入的事务属性以Properties的形式出现，把从BeanDefinition中读到的事务管理的属性信息注入到TransactionInteceptor中。这里创建Spring Aop对事务的Advisor，如果pointcut不为null，则使用默认的通知器,并为通知器配置事务处理拦截器。如果没有配置pointcut，使用TransactionAttributeSourceAdvisor作为通知器，并为通知器设置TransactionInterceptor作为拦截器。TransactionProxyFactoryBean的继承关系从对createMainInterceptor方法的调用分析中可以看到，这个createMainInterceptor方法在IOC容器完成Bean的依赖注入时，通过initializeBean方法被调用。具体调用过程如下图.createMainInterceptor方法的调用关系TransactionProxyFactoryBean通过继承AbstractSingletonProxyFactoryBean，使用afterPropertiesSet()方法，在方法中使用ProxyFactory完成AOP的基本功能.","categories":[],"tags":[]},{"title":"spring的事务切面（中）","slug":"spring的事务切面（中）","date":"2019-02-23T06:28:14.000Z","updated":"2019-03-02T12:47:27.639Z","comments":true,"path":"2019/02/23/spring的事务切面（中）/","link":"","permalink":"http://yoursite.com/2019/02/23/spring的事务切面（中）/","excerpt":"在创建当前线程时，线程中已经有事务存在了 先来复习以下事务的传播特性。 事务的7种传播特性Propagation （事务的传播属性）Propagation：key属性确定代理应该给哪个方法增加事务行为。这样的属性最重要的部份是传播行为。有以下选项可供使用：PROPAGATION_REQUIRED – 支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择。PROPAGATION_SUPPORTS – 支持当前事务，如果当前没有事务，就以非事务方式执行。PROPAGATION_MANDATORY – 支持当前事务，如果当前没有事务，就抛出异常。PROPAGATION_REQUIRES_NEW – 新建事务，如果当前存在事务，把当前事务挂起。PROPAGATION_NOT_SUPPORTED – 以非事务方式执行操作，如果当前存在事务，就把当前### 事务挂起。PROPAGATION_NEVER – 以非事务方式执行，如果当前存在事务，则抛出异常。PROPAGATION_NESTED – 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则进行与PROPAGATION_REQUIRED类似的操作。","text":"在创建当前线程时，线程中已经有事务存在了 先来复习以下事务的传播特性。 事务的7种传播特性Propagation （事务的传播属性）Propagation：key属性确定代理应该给哪个方法增加事务行为。这样的属性最重要的部份是传播行为。有以下选项可供使用：PROPAGATION_REQUIRED – 支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择。PROPAGATION_SUPPORTS – 支持当前事务，如果当前没有事务，就以非事务方式执行。PROPAGATION_MANDATORY – 支持当前事务，如果当前没有事务，就抛出异常。PROPAGATION_REQUIRES_NEW – 新建事务，如果当前存在事务，把当前事务挂起。PROPAGATION_NOT_SUPPORTED – 以非事务方式执行操作，如果当前存在事务，就把当前### 事务挂起。PROPAGATION_NEVER – 以非事务方式执行，如果当前存在事务，则抛出异常。PROPAGATION_NESTED – 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则进行与PROPAGATION_REQUIRED类似的操作。 1： PROPAGATION_REQUIRED加入当前正要执行的事务不在另外一个事务里，那么就起一个新的事务比如说，ServiceB.methodB的事务级别定义为PROPAGATION_REQUIRED, 那么由于执行ServiceA.methodA的时候，ServiceA.methodA已经起了事务，这时调用ServiceB.methodB，ServiceB.methodB看到自己已经运行在ServiceA.methodA的事务内部，就不再起新的事务。而假如ServiceA.methodA运行的时候发现自己没有在事务中，他就会为自己分配一个事务。这样，在ServiceA.methodA或者在ServiceB.methodB内的任何地方出现异常，事务都会被回滚。即使ServiceB.methodB的事务已经被提交，但是ServiceA.methodA在接下来fail要回滚，ServiceB.methodB也要回滚2： PROPAGATION_SUPPORTS如果当前在事务中，即以事务的形式运行，如果当前不再一个事务中，那么就以非事务的形式运行3： PROPAGATION_MANDATORY必须在一个事务中运行。也就是说，他只能被一个父事务调用。否则，他就要抛出异常4： PROPAGATION_REQUIRES_NEW比如我们设计ServiceA.methodA的事务级别为PROPAGATION_REQUIRED，ServiceB.methodB的事务级别为PROPAGATION_REQUIRES_NEW，那么当执行到ServiceB.methodB的时候，ServiceA.methodA所在的事务就会挂起，ServiceB.methodB会起一个新的事务，等待ServiceB.methodB的事务完成以后，他才继续执行。他与PROPAGATION_REQUIRED 的事务区别在于事务的回滚程度了。因为ServiceB.methodB是新起一个事务，那么就是存在两个不同的事务。如果ServiceB.methodB已经提交，那么ServiceA.methodA失败回滚，ServiceB.methodB是不会回滚的。如果ServiceB.methodB失败回滚，如果他抛出的异常被ServiceA.methodA捕获，ServiceA.methodA事务仍然可能提交。5： PROPAGATION_NOT_SUPPORTED当前不支持事务。比如ServiceA.methodA的事务级别是PROPAGATION_REQUIRED ，而ServiceB.methodB的事务级别是PROPAGATION_NOT_SUPPORTED ，那么当执行到ServiceB.methodB时，ServiceA.methodA的事务挂起，而他以非事务的状态运行完，再继续ServiceA.methodA的事务。6： PROPAGATION_NEVER不能在事务中运行。假设ServiceA.methodA的事务级别是PROPAGATION_REQUIRED， 而ServiceB.methodB的事务级别是PROPAGATION_NEVER ，那么ServiceB.methodB就要抛出异常了。7： PROPAGATION_NESTED理解Nested的关键是savepoint。他与PROPAGATION_REQUIRES_NEW的区别是，PROPAGATION_REQUIRES_NEW另起一个事务，将会与他的父事务相互独立，而Nested的事务和他的父事务是相依的，他的提交是要等和他的父事务一块提交的。也就是说，如果父事务最后回滚，他也要回滚的。而Nested事务的好处是他有一个savepoint。在创建当前事务时，线程中已经有事务存在了。这种情况同样需要处理，在声明式事务处理中，在当前线程调用事务方法的时候，就会考虑事务的创建处理，这个处理在方法handleExistingTransaction中完成的。408.如果当前线程已有事务存在，且当前事务的传播属性设置是never，那么抛出异常，说明这种情况是有问题，Spring无法处理当前的事务创建。413.如果当前事务的配置属性是PROPAGATION_NOT_SUPPORTED,同时当前线程已经存在事务了，那么将事务挂起。419.注意PROPAGATION_NOT_SUPPORTED时候的参数，transaction为null, newTransaction为false,意味着事务方法不需要放在事务环境中执行，同时挂起事务的信息记录也保存在TransactionStatus中，这里包括了进程ThreadLocal对事务信息的记录423.如果当前事务的配置属性是PROPAGATION_REQUIRES_NEW，创建新事务，同时把当前线程中存在的事务挂起。与创建全新事务的过程类似，区别在于，在创建全新事务时不用考虑已有事务的挂起，但在这里，需要考虑已有事务的挂起处理。将事务挂起的目的当然是为了在当前事务执行完毕后再将原事务还原。447.嵌套事务的创建457.如果可以使用保存点的方式控制事务回滚，那么在嵌入式事务的建立初始建立保存点，作为异常处理的回滚。466.有些情况是不能使用保存点操作，比如JTA，那么建立新事务，处理方法和PROPAGATION_REQUIRES_NEW相同，而一旦出现异常，则由Spring的事务异常处理机制去完成后续操作。482.这里判断在当前事务方法中的属性配置与已有事务的属性配置是否一致，如果不一致，那么不执行事务方法并抛出异常。501.返回TransactionStatus，注意第三个参数false代表当前事务方法没有使用新的事务。事务的挂起 事务的挂起牵扯到线程与事务处理信息的保存。577.返回的SuspendedResourceHolder会作为参数传给TransactionStatus582.把挂起事务的处理交给具体事务处理器去完成，如果具体的事务处理器不支持事务挂起，那么默认抛出异常TransactionSuspensionNotSupportedException585.这里在线程中保存与事务处理相关的信息，并重置线程中相关的ThreadLocal变量597.方法失败，而初始的事务依然存在基于以上内容，就可以完成声明式事务处理的创建了。事务的提交 事务提交的入口调用在TransactionInteceptor的invoke方法中实现，如commitTransactionAfterReturning(txInfo); 在这个调用中，我们看到的txInfo是TransactionInfo对象，这个参数TransactionInfo对象是创建事务时生成的。同时，Spring的事务管理框架生成的TransactionStatus对象就包含在TransactionInfo对象中。这个commitTransactionAfterReturning方法通过调用事务处理器来完成事务的提交。 在AbstractPlatformTransactionManger中也有一个模板方法支持具体的事务处理器对事务提交的实现。在AbstractPlatformTransactionManager中，这个模板方法的实现与前面的getTransaction类似，如下图707.在TransactionStatus中标识事务已经结束713.如果事务处理过程中发生了异常，调用回滚717.这里处理回滚727.抛出UnexpectedRollbackException异常734.处理提交的入口747.事务提交的准备工作由具体的事务处理器来完成755.这里是嵌套事务的处理761.下面对根据当前线程中保存的事务状态进行处理，如果当前的事务是一个新事务，调用具体事务处理器的完成提交；如果当前所持有的事务不是一个新事务，则不提交，由已经存在的事务来完成提交。807.触发afterCommit()回滚可以看到，事务提交的准备都是由具体的事务处理器来实现的。当然，对这些事务提交的处理，需要通过对TransactionStatus保存的事务处理的相关状态进行判断。提交过程涉及AbstractPlatformTransactionManager中的doCommit和prepareForCommit方法，它们都是抽象方法，都在具体的事务处理器中完成实现。在提交的过程中也并不是直接提交的，而是考虑了诸多的方面，符合提交的条件如下：1.当事务状态中有保存点信息的话不会去提交事务2.当事务非新事务的时候也不会去执行提交事务操作 事务的回滚在对目标方法的执行过程中，一旦出现Throwable就会被引导至此方法处理，但是并不代表所有的Throwable就会被回滚处理。关键就在于txInfo.transactionAttribute.rollBackOn(ex)。当然你可以通过扩展改变，例如@Transactional(propagation= Propagation.REQUIRED,rollbackFor = Exception.class)843.根据TransactionStatus信息进行回滚处理847.嵌套事务的回滚处理851.如果有保存点，也就是当前事务为单独的线程则会退到保存点853.如果当前事务为独立的新事务，则直接回退859.如果当前事务不是独立的事务，那么只能标记状态，等到事务链执行完毕后统一回滚866.由线程中的前一个事务来处理回盾，这里不执行任何操作887.清空记录的资源并将挂起的资源恢复对于回滚逻辑执行结束后，无论回滚是否成功，都必须要做的事情就是事务结束后的收尾工作。事务处理的收尾处理工作包括以下内容： * 设置状态时对事务信息作完成标识以避免重复调用。 * 如果当前事务是新的同步状态，需要将绑定到当前线程的事务信息清除。 * 如果是新事务需要做些清除资源的工作 * 如果在事务执行前有事务挂起，那么当前事务执行结束后需要将挂起事务恢复。351.将数据库连接从当前线程中解除绑定355.释放链接358.恢复数据库连接的自动提交属性360.重置数据库连接370.如果当前事务是独立的新创建的事务，则在事务完成时释放数据库连接 Spring事务处理器的设计与实现以DataSourceTransactionManager为例，在DataSourceTransactionManager中，在事务开始的时候，会调用doBegin方法，首先会得到相对应的Connection，然后可以根据事务设置的需要，对Connection的相关属性进行配置，比如将Connection的autoCommit功能关闭，并对像TimeoutInSeconds这样的事务处理参数进行设置，最后通过TransactionSynchronizationManager来对资源进行绑定。实现DataSourceTransactionManager的时序图 155.这是注入的dataSource220.这里是产生Transaction的地方，为Transaction的创建提供服务。对数据库而言，事务工作是由Connection来完成。这里把数据库的Connection对象放到一个ConnectionHolder中，然后封装到一个DataSourceTransactionObject对象中，在这个封装过程中增加了许多为事务处理服务的控制数据。223.获取与当前线程绑定的数据库Connection，这个Connection在第一个事务开始的地方与线程绑定。这里是判断是否已经存在事务的地方，由ConnectionHolder的isTransactionActive属性来控制。239.这里是处理事务开始的地方。构造transaction,包括设置ConnectionHolder、隔离级别、timeout。如果是新连接，绑定到当前线程。256.设置隔离级别。设置隔离级别的prepareConnectionForTransaction函数用于负责对底层数据库连接的设置，当然，只是包含只读标识和隔离级别的设置。隔离级别最终的控制也是交由connection。262.这里是数据库Connection完成事务处理的重要配置，需要把autoCommit属性关掉，由Spring控制提交。271.设置判断当前线程是否存在事务的依据。279.把当前的数据库Connection和线程绑定 可以说事务是从doBegin()这个函数开始的，因为在这个函数中已经开始尝试了对数据库连接的获取，当然，在获取数据库连接的同时，一些必要的设置也是需要同步设置的。 * 尝试获取连接。当然并不是每次都会获取新的连接，如果当前线程中的connectionHolder已经存在，则没必要再次获取，或者，对于事务同步标识设置为true的需要重新获取连接。 * 设置隔离级别以及只读标志。Spring中确实是针对只读操作做了一些处理，但是核心的实现是设置connection上的readOnly属性。同时，对于隔离级别的控制也是交由connection去控制的。 * 更改默认的提交设置。 * 设置标志位，标识当前连接已经被事务激活。 * 设置过期时间。 * 将connectionHolder绑定到当前线程 306.事务的提交过程307.取得Connection以后，通过Connection进行提交321.事务的回滚过程，使用Connection的rollback方法 上面介绍了使用DataSourceTransactionManager实现事务创建、提交和回滚的过程，基本上与单独使用Connection实现事务处理是一样的，也是通过设置autoCommit属性，调用Connection的commit和rollback方法来完成的。 整体总结：在以上过程中，有几个Spring事务处理的核心类是我们需要关注的。其中包括：TransactionInterceptor——它是使用AOP实现声明式事务处理的拦截器，封装了Spring对声明式事务处理实现的基本过程TransactionAttributeSource和TransactionAttribute——它们封装了对声明式事务处理属性的识别，以及信息的读入和配置。我们看到的TransactionAttribute对象，可以视为对事务处理属性的数据抽象，如果在使用声明式事务处理的时候，应用没有配置这些属性，Spring将为用户提供DefaultTransactionAttribute对象，在这个DefaultTransactionAttribute对象中，提供了默认的事务处理属性设置。 在事务处理过程中，可以看到TransactionInfo和TransactionStatus这两个对象，它们视存放事务处理信息的主要数据对象，它们通过与线程的绑定来实现事务的隔绝性。具体来说，TransactionInfo对象本身就像是一个栈，对应着每一次事务方法的调用，它会保存每一次事务方法调用的事务处理信息。值得注意的是，在TransactionInfo对象中，它持有TransactionStatus对象，这个TransactionStatus是非常重要的。由这个TransactionStatus来掌管事务执行的详细信息，包括具体的事务对象、事务执行状态、事务设置状态等。在事务的创建、启动、提交和回滚的过程中，都需要与这个TransactionStatus对象中的数据打交道。在准备完这些与事务管理有关的数据之后，具体的事务处理是由事务处理器TransactionManager来完成的。在事务处理器完成事务处理的过程中，与具体事务处理器无关的操作都被封装到AbstractionPlatformTransactionManager中实现了。这个抽象的事务处理器为不同的具体事务处理器提供了通用的事务处理模板，它封装了在事务处理过程中，与具体事务处理器无关的公共的事务处理部分。我们在具体的事务处理器(比如DataSourceTransactionManager)的实现中可以看到，最为底层的事务创建、挂起、提交、回滚操作。","categories":[],"tags":[]},{"title":"spring的事务切面（上）","slug":"spring的事务切面（上）","date":"2019-02-23T06:18:38.000Z","updated":"2019-03-02T12:38:42.184Z","comments":true,"path":"2019/02/23/spring的事务切面（上）/","link":"","permalink":"http://yoursite.com/2019/02/23/spring的事务切面（上）/","excerpt":"一个问题在插入操作的事务最后抛出Exception异常，能否插入成功？Spring事务处理的设计概览 Spring的事务处理模板中的类层次接口 真正处理事务的是TransactionInterceptor，PlatformTransactionManager,AbstractionTransactionManager以及DataSourceTransactionManager，其他的类用来读取配置、加载通知以及实现织入。事务处理拦截器的设计与实现TransactionInterceptor类继承自MethodInterceptor，所以调用该类是从其invoke方法开始的。这个invoke()方法是Proxy代理对象的回调方法，在调用Proxy对象的代理方法时触发这个回调。在事务处理拦截器TransactionInterceptor中，invoke方法的实现如下图。可以看到，其过程时，首先获得调用方法的事务处理配置；在得到事务处理配置之后，会取得配置的PlatformTransactionManger，由这个事务处理器来实现事务的创建、提交、回滚操作。PlatformTransactionManger事务处理器是在Ioc容器中配置的，比如大家已经很熟悉的DataSourceTransactionManger和HibernateTransactionManager。有了这一系列的具体事务处理器的配置，在Spring事务处理模块的统一管理下，由这些具体的事务处理器来完成事务的创建、提交、回滚等底层的事务操作。","text":"一个问题在插入操作的事务最后抛出Exception异常，能否插入成功？Spring事务处理的设计概览 Spring的事务处理模板中的类层次接口 真正处理事务的是TransactionInterceptor，PlatformTransactionManager,AbstractionTransactionManager以及DataSourceTransactionManager，其他的类用来读取配置、加载通知以及实现织入。事务处理拦截器的设计与实现TransactionInterceptor类继承自MethodInterceptor，所以调用该类是从其invoke方法开始的。这个invoke()方法是Proxy代理对象的回调方法，在调用Proxy对象的代理方法时触发这个回调。在事务处理拦截器TransactionInterceptor中，invoke方法的实现如下图。可以看到，其过程时，首先获得调用方法的事务处理配置；在得到事务处理配置之后，会取得配置的PlatformTransactionManger，由这个事务处理器来实现事务的创建、提交、回滚操作。PlatformTransactionManger事务处理器是在Ioc容器中配置的，比如大家已经很熟悉的DataSourceTransactionManger和HibernateTransactionManager。有了这一系列的具体事务处理器的配置，在Spring事务处理模块的统一管理下，由这些具体的事务处理器来完成事务的创建、提交、回滚等底层的事务操作。第一步是得到代理的目标对象，并将事务属性传递给目标对象。采用非回调的方法来对事务进行提交271.读取事务的配置属性，通过TransactionAttributeSource对象取得272.根据TransactionProxyFactoryBean的配置信息获得具体的事务处理器273.构造方法唯一标识（类.方法 如service.impl.UserServiceImpl.save）275.这里区分不同类型的PlatformTransactionManger，因为他们的调用方式不同，对CallbackPreferringPlatformTransactionManger来说，需要回调函数来实现事务的创建和提交；对于非CallbackPreferringPlatformTransactionManger来说，不需要通过回调函数来实现事务的和提交，像DataSourceTransactionManger就不是CallbackPreferringPlatformTransactionManger，不需要通过回调的方式来使用。277.这里创建事务，同时把创建事务过程中得到的信息放到TransactionInfo中去，TransactionInfo是保存当前事务状态的对象。282.这里的调用使处理沿着拦截器链进行，使最后目标对象的方法得到调用。286.如果在事务处理方法调用中出现了异常，事务处理如何进行需要根据具体的情况考虑回滚或者提交。Spring默认只对RuntimeException以及Error执行回滚290.这里把与线程绑定的TransactionInfo设置为oldTransactionInfo292.这里通过事务处理器来对事务进行提交。采用回调的方法来使用事务处理器。312.RuntimeException会导致事务回滚321.正常的返回，导致事务提交流程梳理：在调用代理的事务方法时，因为前面已经完成了一系列AOP配置，对事务方法的调用，最终启动TransactionInterceptor拦截器的invoke方法。在这个方法中，首先会读取该事务方法的事务属性配置，然后根据事务属性配置以及具体事务处理器的配置来决定采用哪一个事务处理器，这个事务处理器实际上是一个PlatformTransactionManger。在决定好具体的事务处理器之后，会根据事务的运行情况和事务配置来决定是不是需要创建新的事务。对于Spring而言，事务的管理实际上是通过一个TransactionInfo对象来完成的，在该对象中，封装了事务对象和事务处理的状态信息，这是事务处理的抽象。在这一步完成之后，会对拦截器链进行处理，因为有可能在该事务对象中还配置了除事务处理AOP之外的其他拦截器。在结束对拦截器链处理之后，会对TransactionInfo中的信息进行更新，以反映最近的事务处理情况，在这个时候，也就是完成了事务提交的准备，通过调用事务处理器PlatformTransactionManger的commitTransactionAfterReturning方法来完成事务的提交。这个提交的处理过程已经封装在PlatformTransactionManger的事务处理器中了，而与具体数据源相关的处理过程，最终委托给相关的具体事务处理器来完成，比如DataSourceTransactionManager、HibernateTransactionManager等。事务提交的时序图在这个invoke()方法的实现中，可以看到整个事务处理在AOP拦截器中实现的全过程。同时，它也是Spring采用AOP封装事务处理和实现声明式事务处理的核心部分。这部分实现，是一个桥梁，它胶合了具体的事务处理和Spring AOP框架，可以看成是一个Spring AOP应用，在这个桥梁搭建完成之后，Spring事务处理的实现就开始了。Spring事务处理的编程式使用（模型）在编程式使用事务处理的过程中，利用DefaultTransactionDefinition对象来持有事务处理属性。同时，在创建事务的过程中得到一个TransactionStatus对象，然后通过直接调用transactionManager的commit()和rollback()方法来完成事务处理。在这个编程式使用事务管理的过程中，没有看到框架特性的使用，非常简单和直接，很好地说明了事务管理的基本实现过程，以及在Spring事务处理实现中涉及一些主要的类，比如TransactionStatus、TransactionManger等，对这些类的使用与声明式事务处理的最终实现是一样的。事务的创建作为声明式事务处理实现的起始点，需要注意TransactionInterceptor拦截器的invoke回调中使用的createTransactionIfNecessary方法，这个方法是在TransactionIntercepor的基类TransactionAspectSupport中实现的。在createTransactionIfNecessary方法的调用中，会向AbstractTransactionManger执行getTransaction()，这个获取Transaction事务对象的过程，在AbstractTransactionManger实现中需要对事务的情况做出不同的处理，然后，创建一个TransactionStatus，并把这个TransactionStatus设置到对应的TransactionInfo中去，同时将TransactionInfo和当前的线程绑定，从而完成事务的创建过程。调用createTransactionIfNecessary的时序图445.从外部参数读取事务方法调用的事务配置属性，以及使用的PlatformTransactionManger449.如果没有指定名字，使用方法唯一标识来作为事务名458.这个TransactionStatus封装了事务执行的状态信息461.这里使用了定义好的事务方法的配置信息，事务创建由事务处理器来完成，同时返回TransactionStatus来记录当前的事务状态，包括已经创建的事务470.准备TransactionInfo。TransactionInfo对象封装了事务处理的配置信息以及TransactionStatus491.这里为TransactionInfo设置TransactionStatus，这个TransactionStatus很重要，它持有管理事务处理需要的数据，比如，transaction对象就是由TransactionStatus来持有的。504.这里把当前的TransacctionInfo与线程绑定，同时在TransactionInfo中由一个变量来保存以前的TransactionInfo，这样就持有了一连串与事务处理相关的TransactionInfo，虽然不一定需要创建新的事务，但是总会在请求事务时创建TransactionInfo。 具体的事务创建可以交给事务处理器来完成。在事务的创建过程中，已经为事务的管理做好了准备，包括记录事务处理状态，以及绑定事务信息和线程等。createTransactionIfNecessary()方法中的tm.getTransaction(txAttr)调用触发，生成一个TransactionStatus对象，封装了底层事务对象的创建。可以看到，AbstractionPlatformTransactionManager提供了创建事务的模板。AbstractPlatformTransactionManager会根据事务属性配置和当前进程绑定的事务信息，对事务是否需要创建，怎样创建进行一些通用的处理，然后把事务创建的底层工作交给具体的事务处理器完成。尽管具体的事务处理器完成事务创建的过程各不相同，但是不同的事务处理器对事务属性和当前进程事务信息的处理都是相同的。341.这个doGetTransaction()是抽象函数，Transaction对象的取得由具体的事务处理器实现，比如DataSourceTransactionManager344.缓存debug标志位346.如果没有设置事务属性，那么使用默认的事务属性DefaultTransactionDefinition。关于这个DefaultTransactionDefinition，在前面编程式使用事务处理的时候遇到过。这个DefaultTransactionDefinition的默认事务处理属性是：propagationBehivor = PROPAGATION_REQUIRED；isolationLevel=ISOLATION_DEFAULT；timeout=TIMEOUT_DEFAULT;readOnly=false351.检查当前线程是否已经存在事务，如果已经存在事务，那么需要根据在事务属性中定义的事务传播属性配置来处理事务的产生（handleExistingTransaction()）。353.这里对当前线程中已经有事务存在的情况进行处理，结果封装在TransactionStatus中。357.检查事务属性中timeout的设置是否合理361.当前没有事务存在，这是需要根据事务属性设置来创建事务，这里会看到对事务传播属性设置的处理，比如mandatory、required、required_new、nested等。377.这里是创建事务的调用，构造transaction，包括设置ConnectionHolder、隔离级别、timeout。如果是新连接，绑定到当前线程。由具体的事务处理器完成，比如HibernateTransactionManager和DataSourceTransactionManager等。在后面DataSourceTransactionManager的说明中会进一步讲解。379.返回TransactionStatus封装事务执行情况，默认getTransactionSynchronization = SYNCHRONIZATION_ALWAYS，所以在这种情况下，newSynchronization为true391.创建TransactionStatus，但是没有transaction对象，因为在newTransactionStatus中对应于transaction的参数是null流程梳理： 事务创建的结果是生成一个TransactionStatus对象，通过这个对象来保存事务处理需要的基本信息，这个对象与前面提到过的TransactionInfo对象联系在一起，TransactionStatus是TransactionInfo的一个属性，然后会把TransactionInfo保存在ThreadLocal对象里，这样当前线程可以通过ThreadLocal对象取得TransactionInfo，以及与这个事务对应的TransactionStatus对象，从而把事务的处理信息与调用事务方法的当前线程绑定起来。528.这里判断是不是新事务，如果是新事务，那么需要把事务属性存放到当前线程中。TransactionSynchronizationManager维护一系列的ThreadLocal变量来保持事务属性，比如并发事务隔离级别，是否有活跃的事务等。530.这里是把结果记录在DefaultTransactionStatus中返回。 新事务的创建是比较好理解的，这里需要根据事务属性配置进行创建。所谓创建，首先是把创建工作交给具体的事务处理器来完成，比如DataSourceTransactionManager，把创建的事务对象在TransactionStatus中保存下来，然后将其他的事务属性和线程ThreadLocal变量进行绑定。","categories":[],"tags":[]},{"title":"也谈HashMap实现原理","slug":"也谈HashMap实现原理","date":"2019-02-17T06:20:53.000Z","updated":"2019-02-17T10:57:00.938Z","comments":true,"path":"2019/02/17/也谈HashMap实现原理/","link":"","permalink":"http://yoursite.com/2019/02/17/也谈HashMap实现原理/","excerpt":"","text":"HashMap在互联网应用中是一个老生常谈的话题。从redis等各类缓存到spring内部BeanDefinition的存储，都能看到HashMap的身影。可见hashMap这个数据结构的重要性。最近也在重看HashMap的结构，记录一下，希望能有一些启发。 不同版本的JDK,HashMap的实现略有不同，以下是JDK1.8版本的HashMap。用intelliJ打开HashMap源码，打开Structure视图,可以看到HashMap主体存储结构是1234567/** * The table, initialized on first use, and resized as * necessary. When allocated, length is always a power of two. * (We also tolerate length zero in some operations to allow * bootstrapping mechanics that are currently not needed.) */transient Node&lt;K,V&gt;[] table; Node是HashMap的一个静态内部类，以下是Node结构的定义123456789101112static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; Node结构中的next字段指向的是下一个Node节点，所以简单来说，HashMap是一个桶型的结构,如图其中，数组是基本结构，数组中存储的是链表。而HashMap的查找顺序是首先按照hash值，找到数组中的链表，然后遍历这个链表，通过key对象的equals方法逐一进行比对。 再来看一下主要的几个字段1234final float loadFactor; //table的负载因子，负载因子越高，table的填充率越高static final float DEFAULT_LOAD_FACTOR = 0.75f; //table负载因子默认是0.75transient int size; //map中的k-v键值对数量int threshold; //容纳k-v对的极限，如果超过这个值，就会扩容 看主要的初始化方法123456789101112public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125; 可以看到，这里并没有对table分配内存空间，分配的内存操作放在put方法中，这里要注意tableSizeFor(initialCapacity)这个方法，源码如下：123456789static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 可以看到，这里用了一个位运算，无符号右移，最终的作用就是输出不小于cap的首个2的n次幂，作为table的初始大小。至于为什么HashMap的数组长度要用2的n次幂，这个后面会有讲解。","categories":[],"tags":[]},{"title":"yilia主题的hexo如何集成gitalk评论插件","slug":"yilia主题的hexo如何集成gitalk评论插件","date":"2019-02-11T08:20:20.000Z","updated":"2019-02-11T11:15:06.382Z","comments":true,"path":"2019/02/11/yilia主题的hexo如何集成gitalk评论插件/","link":"","permalink":"http://yoursite.com/2019/02/11/yilia主题的hexo如何集成gitalk评论插件/","excerpt":"","text":"gitalk是一个利用Github API,基于Github issue和Preact开发的评论插件，使用github账号登录,支持markdown语法,和hexo集成也比较简单 阅读这篇文章的朋友注意了，本文是基于yilia主题的hexo，如果是其他主题，可能配置会有一些差异 第一、首先需要在github上新建一个OAuth Apps，方法是在右上角头像下的settings-&gt;developer settings新建 第二、在hexo安装目录的\\themes\\yilia\\layout_partial\\post文件夹中新建gitalk.ejs,代码如下12345678910111213141516&lt;div class=&quot;gitalk&quot;&gt; &lt;div id=&quot;gitalk-container&quot;&gt;&lt;/div&gt; &lt;script src=&quot;/lib/md5/md5.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; const gitalk = new Gitalk(&#123; clientID: &apos;&lt;%=theme.gitalk.client_id%&gt;&apos;, clientSecret: &apos;&lt;%=theme.gitalk.client_secret%&gt;&apos;, repo: &apos;&lt;%=theme.gitalk.repo%&gt;&apos;, owner: &apos;&lt;%=theme.gitalk.owner%&gt;&apos;, admin: &apos;&lt;%=theme.gitalk.admin%&gt;&apos;, id: md5(location.pathname), // Ensure uniqueness and length less than 50 distractionFreeMode: false // Facebook-like distraction free mode &#125;) gitalk.render(&apos;gitalk-container&apos;) &lt;/script&gt;&lt;/div&gt; 其中clientID和clientSecret是上一步注册OAuth Apps后得到的key和secretrepo是存放评论的仓库名，owner和admin是你github的登录名注意这里id有一个md5的函数(这是一个坑，后面会说明)，需要引入1&lt;script src=&quot;/lib/md5/md5.min.js&quot;&gt;&lt;/script&gt; 这个md5的js可以从github上搜索下载到，存放路径是主题source文件夹下，例如hexo\\themes\\yilia\\source\\lib 第三、在hexo\\themes\\yilia\\layout_partial\\article.ejs文件后加上1234567&lt;% if (!index &amp;&amp; theme.gitalk.enable &amp;&amp; post.comments)&#123; %&gt;&lt;%- partial(&apos;post/gitalk&apos;, &#123; key: post.slug, title: post.title, url: config.url+url_for(post.path) &#125;) %&gt;&lt;% &#125; %&gt; 其中post/gitalk是gitalk.ejs文件的相对路径 最后、修改根目录下的_config.yml，在最后加上123456789# 注释所有畅言配置# 配置gitalkgitalk: enable: true client_id: 你申请的clientId client_secret: 你申请的clientSecret repo: 博客仓库的名称 owner: &apos;你github的用户名&apos; admin: &apos;你github的用户名&apos; 最后的最后，重新发布hexo即可 至于上面第二步为啥要加上md5函数，是因为用原始的id，会因为文章名称过长，导致gitalk初始化失败登陆后显示Error: Validation Failed.同时无法进行评论，评论框可以编辑 网页错误信息如下12345Failed to load resource: the server responded with a status of 422 (Unprocessable Entity)gitalk.jsx:127 err: Error: Request failed with status code 422 at e.exports (https://unpkg.com/gitalk@1.2.2/dist/gitalk.min.js:1:34270) at e.exports (https://unpkg.com/gitalk@1.2.2/dist/gitalk.min.js:19:1283) at XMLHttpRequest.h.(anonymous function) (https://unpkg.com/gitalk@1.2.2/dist/gitalk.min.js:1:33269) 另外，如果根目录下_config.yml中的repo参数不对，打开评论页面后会报Error:Not Found错误","categories":[],"tags":[{"name":"博客","slug":"博客","permalink":"http://yoursite.com/tags/博客/"}]},{"title":"JVM性能调优相关主题总结","slug":"JVM性能调优相关主题总结","date":"2019-02-11T06:18:39.000Z","updated":"2019-03-05T05:05:18.142Z","comments":true,"path":"2019/02/11/JVM性能调优相关主题总结/","link":"","permalink":"http://yoursite.com/2019/02/11/JVM性能调优相关主题总结/","excerpt":"","text":"JVM基本 所有线程共享的内存数据区：方法区，堆。 线程私有：虚拟机栈，本地方法栈，程序计数器。 存放于栈中的东西如下： 每个线程包含一个栈区，栈中只保存基础数据类型的对象和自定义对象的引用（不是对象）。对象都存放在堆区中。 每个栈中的数据（基础数据类型和对象引用）都是私有的，其他栈不能访问 方法的形式参数，方法调用后从栈空间回收 引用对象的地址，引用完后，栈空间地址立即被回收，堆空间等GC 存放在堆中的东西如下： 存储的全部都是对象，每个对象包含一个与之对应的class信息. 每创建一个线程就会对应创建一个Java栈，所以Java栈也是线程私有的内存区域。压入栈帧==用来存储方法数据和部分过程结果的数据结构，主线程对应一个栈帧（存储临时变量和参数）JDK1.7及以前的JDK版本，Java类信息、常量池、静态变量都存储在Perm里JDK1.8对JVM架构的改造，将类元数据放在本地内存中，另外，将常量池和静态变量放到Java堆里。HotSpot VM将会为类的元数据明确分配和释放本地内存。 垃圾回收器总结 Serial收集器：可能产生较大停顿，只使用一个线程去回收。新生代、老年代使用串行回收。新生代复制算法，老年代标记-压缩。-XX：UserSerialGC串行收集器。 ParNew收集器：其实就是Serial收集器的多线程版本。新生代并行，老年代串行；新生代复制算法，老年代标记-压缩。 -XX:UserParNewGC 使用ParNew收集器 -XX:ParallelGCThreads 限制线程数量 Parallel收集器：Parallel Scavenge收集器类似ParNew收集器，Parallel收集器更关注系统的吞吐量。可以通过参数打开自适应调节策略，动态调整这些参数以提供最合适的停顿时间或最大的吞吐量；也可以通过参数控制GC的时间不大于多少毫秒或者比例。新生代复制，老年代标记-压缩。 -XX：+UseParallelGC 使用parallel收集器+老年代串行 Parallel Old收集器：是Parallel Scavenge收集器的老年代版本，使用多线程和“标记-整理”算法 -XX:+UseParallelOldGC 使用Parallel收集器+老年代并行。 CMS收集器：是一种以获取最短回收停顿时间为目标的收集器。基于“标记-清除”算法实现的 初始标记 -&gt; stop the world 并发标记 重新标记 -&gt; stop the world 并发清除初始标记仅仅只是标记一下GC Roots能直接关联对象，速度很快，并发标记就是进行GC Roots Tracing的过程，而重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。在耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作。CMS是老年代的收集器（新生代用ParNew）优点：并发手机、低停顿缺点：产生大量碎片，并发阶段会降低吞吐量参数控制：-XX:+UseConcMarkSweepGC 启用CMS收集器-XX:+UseCmsCompactAtFullCollection Full GC后，进行一次碎片整理，独占的，停顿时间变长-XX:+CMSFullGCsBeforeCompaction 设置进行几次full gc后，进行一次碎片整理-XX: ParallelCMSThreads设定CMS的多线程数量-XX:+CMSInitiatingOccupancyFraction=80 CMS垃圾收集器，当老年代达到80%时，触发CMS垃圾回收。 G1收集器 新生代和老年代混合排列。","categories":[],"tags":[]},{"title":"遇到线上服务器Full GC怎么办?","slug":"遇到线上服务器Full-GC怎么办","date":"2019-02-11T06:16:01.000Z","updated":"2019-02-11T06:16:01.190Z","comments":true,"path":"2019/02/11/遇到线上服务器Full-GC怎么办/","link":"","permalink":"http://yoursite.com/2019/02/11/遇到线上服务器Full-GC怎么办/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"记一次线上异常的排查和定位","slug":"记一次线上异常的排查和定位","date":"2019-02-09T12:30:02.000Z","updated":"2019-02-09T15:50:43.191Z","comments":true,"path":"2019/02/09/记一次线上异常的排查和定位/","link":"","permalink":"http://yoursite.com/2019/02/09/记一次线上异常的排查和定位/","excerpt":"","text":"某段时间，线上服务不时会爆出dwr数据异常。初步断定是后端接口返回异常所致。 由于当时我们的服务已经接入ELK，所以第一时间登录kibana后台，按照lucene语法搜索（Tag:”study_online”） AND (message:”exception”)，定位到服务器的异常 异常信息(由于当时的异常日志已经被冲掉了，这是我从网上摘录的)：1234567891011121314java.lang.IllegalStateException: Timed out waiting to add Cmd: 1 Opaque: 1147840 Key: com.stubhub.user.business.entity.UserSession|63C21A07311389A1EC361D834BF46E72 Cas: 0 Exp: 7200 Flags: 1 Data Length: 1748(max wait=10000ms) at net.spy.memcached.protocol.TCPMemcachedNodeImpl.addOp(TCPMemcachedNodeImpl.java:362) at net.spy.memcached.MemcachedConnection.addOperation(MemcachedConnection.java:1267) at com.couchbase.client.CouchbaseConnection.addOperation(CouchbaseConnection.java:277) at net.spy.memcached.MemcachedConnection.enqueueOperation(MemcachedConnection.java:1185) at net.spy.memcached.MemcachedClient.asyncStore(MemcachedClient.java:328) at net.spy.memcached.MemcachedClient.set(MemcachedClient.java:929) at com.stubhub.common.cache.store.couchbase.CouchbaseStore.put(CouchbaseStore.java:148) at com.stubhub.common.cache.store.CompositeCacheStore.put(CompositeCacheStore.java:69) at com.stubhub.common.session.impl.UserSessionCacheManagerImpl.putCrossModuleValue(UserSessionCacheManagerImpl.java:48) at com.stubhub.user.business.manager.impl.UserSessionCacheMgrImpl.putUserSessionToStore(UserSessionCacheMgrImpl.java:269) at com.stubhub.user.business.manager.impl.UserSessionCacheMgrImpl.createUserSession(UserSessionCacheMgrImpl.java:806) at sun.reflect.GeneratedMethodAccessor1305.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 从日志中判断是因为memcached超时抛出的异常。首先查看工程的memcached设置，代码如下123456789101112131415161718192021222324&lt;bean id=&quot;memcachedClient&quot; class=&quot;com.xxx.xxx.cache.impl.KeyPrefixSupportedMemcachedClientFactory&quot;&gt; &lt;property name=&quot;servers&quot; value=&quot;$&#123;memcached_address_list&#125;&quot; /&gt; &lt;property name=&quot;namespace&quot; value=&quot;study_&quot; /&gt; &lt;property name=&quot;protocol&quot; value=&quot;BINARY&quot; /&gt; &lt;property name=&quot;transcoder&quot;&gt; &lt;bean class=&quot;net.spy.memcached.transcoders.SerializingTranscoder&quot;&gt; &lt;property name=&quot;compressionThreshold&quot; value=&quot;16384&quot; /&gt; &lt;/bean&gt; &lt;/property&gt; &lt;property name=&quot;maxReconnectDelay&quot; value=&quot;60&quot; /&gt; &lt;property name=&quot;opTimeout&quot; value=&quot;200&quot;/&gt; &lt;property name=&quot;opQueueMaxBlockTime&quot; value=&quot;400&quot;/&gt; &lt;property name=&quot;timeoutExceptionThreshold&quot; value=&quot;20&quot;/&gt; &lt;property name=&quot;hashAlg&quot;&gt; &lt;value type=&quot;net.spy.memcached.DefaultHashAlgorithm&quot;&gt;KETAMA_HASH&lt;/value&gt; &lt;/property&gt; &lt;property name=&quot;locatorType&quot;&gt; &lt;value type=&quot;net.spy.memcached.ConnectionFactoryBuilder.Locator&quot;&gt;CONSISTENT&lt;/value&gt; &lt;/property&gt; &lt;property name=&quot;failureMode&quot;&gt; &lt;value type=&quot;net.spy.memcached.FailureMode&quot;&gt;Redistribute&lt;/value&gt; &lt;/property&gt; &lt;property name=&quot;useNagleAlgorithm&quot; value=&quot;false&quot; /&gt;&lt;/bean&gt; 可以看到超时时间opTimeout设置的是200毫秒，而opQueueMaxBlockTime是400毫秒opTimeout是操作超时时间，opQueueMaxBlockTime是指操作的最大阻塞时间。那究竟是哪个时间超时导致的呢？可以通过下载memcacheClient源码，搜索“Timed out waiting to add”定位到如下源码12345678910111213141516171819202122232425/* * (non-Javadoc) * * @see net.spy.memcached.MemcachedNode#addOp(net.spy.memcached.ops.Operation) */public final void addOp(Operation op) &#123; try &#123; if (!authLatch.await(1, TimeUnit.SECONDS)) &#123; op.cancel(); getLogger().warn(&quot;Operation canceled because authentication &quot; + &quot;or reconnection and authentication has &quot; + &quot;taken more than one second to complete.&quot;); getLogger().debug(&quot;Canceled operation %s&quot;, op.toString()); return; &#125; if (!inputQueue.offer(op, opQueueMaxBlockTime, TimeUnit.MILLISECONDS)) &#123; throw new IllegalStateException(&quot;Timed out waiting to add &quot; + op + &quot;(max wait=&quot; + opQueueMaxBlockTime + &quot;ms)&quot;); &#125; &#125; catch (InterruptedException e) &#123; // Restore the interrupted status Thread.currentThread().interrupt(); throw new IllegalStateException(&quot;Interrupted while waiting to add &quot; + op); &#125;&#125; 从源码中可以知晓，原因是inputQueue.offer没有正常返回导致的，超过了opQueueMaxBlockTime的时间限制 知道了原因，下面就开始思考怎么解决吧","categories":[],"tags":[{"name":"debug","slug":"debug","permalink":"http://yoursite.com/tags/debug/"}]},{"title":"Markdown基本语法","slug":"Markdown基本语法","date":"2019-02-09T10:32:02.000Z","updated":"2019-02-09T12:51:20.911Z","comments":true,"path":"2019/02/09/Markdown基本语法/","link":"","permalink":"http://yoursite.com/2019/02/09/Markdown基本语法/","excerpt":"","text":"一、标题1234567语法# 这是一级标题## 这是二级标题### 这是三级标题#### 这是四级标题##### 这是五级标题###### 这是六级标题 示例 这是一级标题这是二级标题这是三级标题这是四级标题这是五级标题这是六级标题二、字体12345语法**这是加粗的文字***这是倾斜的文字*`***这是斜体加粗的文字***~~这是加删除线的文字~~ 示例这是加粗的文字这是倾斜的文字`这是斜体加粗的文字这是加删除线的文字 三、引用1234语法&gt;这是引用的内容&gt;&gt;这是引用的内容&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;这是引用的内容 示例 这是引用的内容 这是引用的内容 这是引用的内容 四、分割线12345语法-------******** 示例 五、图片1234语法：![图片alt](图片地址 &apos;&apos;图片title&apos;&apos;)图片alt就是显示在图片下面的文字，相当于对图片内容的解释。图片title是图片的标题，当鼠标移到图片上时显示的内容。title可加可不加 示例 六、超链接123语法：[超链接名](超链接地址 &quot;超链接title&quot;)title可加可不加 示例：简书百度 七、列表 无序列表 12语法：无序列表用 - + * 任何一种都可以 有序列表 12语法：数字加点 例如1.列表内容2.列表内容3.列表内容 列表嵌套 一级无序列表内容 二级无序列表内容 二级无序列表内容 二级无序列表内容 一级无序列表内容 1.二级有序列表内容 2.二级有序列表内容 3.二级有序列表内容 1.一级有序列表内容 二级无序列表内容 二级无序列表内容 二级无序列表内容 2.一级有序列表内容 1.二级有序列表内容 2.二级有序列表内容 3.二级有序列表内容 八、表格123456789101112语法：表头|表头|表头---|:--:|---:内容|内容|内容内容|内容|内容第二行分割表头和内容。- 有一个就行，为了对齐，多加了几个文字默认居左-两边加：表示文字居中-右边加：表示文字居右注：原生的语法两边都要用 | 包起来。此处省略 示例：表头|表头|表头—|:–:|—:内容|内容|内容内容|内容|内容 九、代码语法：单行代码：代码之间分别用一个反引号包起来create table user;代码块：代码之间分别用三个反引号包起来，且两边的反引号单独占一行123代码...代码...代码...","categories":[],"tags":[{"name":"个人博客","slug":"个人博客","permalink":"http://yoursite.com/tags/个人博客/"}]},{"title":"TestNg框架源码解析","slug":"TestNg框架源码解析","date":"2019-01-01T04:08:58.000Z","updated":"2019-03-05T05:01:58.168Z","comments":true,"path":"2019/01/01/TestNg框架源码解析/","link":"","permalink":"http://yoursite.com/2019/01/01/TestNg框架源码解析/","excerpt":"","text":"TestNG是一个测试框架，其灵感来自JUnit和NUnit，但引入了一些新的功能，使其功能更强大，使用更方便。TestNG是一个开源自动化测试框架;TestNG表示下一代(Next Generation的首字母)。 TestNG类似于JUnit(特别是JUnit 4)，但它不是JUnit框架的扩展。它的灵感来源于JUnit。它的目的是优于JUnit，尤其是在用于测试集成多类时。TestNG消除了大部分的旧框架的限制，使开发人员能够编写更加灵活和强大的测试。 因为它在很大程度上借鉴了Java注解(JDK5.0引入的)来定义测试，它也可以显示如何使用这个新功能在真实的Java语言生产环境中。 TestNG的特点l 注解l TestNG使用Java和面向对象的功能l 支持综合类测试(例如，默认情况下，不用创建一个新的测试每个测试方法的类的实例)l 独立的编译时测试代码和运行时配置/数据信息l 灵活的运行时配置l 主要介绍“测试组”。当编译测试，只要要求TestNG运行所有的“前端”的测试，或“快”，“慢”，“数据库”等l 支持依赖测试方法，并行测试，负载测试，局部故障l 灵活的插件APIl 支持多线程测试123456&lt;dependency&gt; &lt;groupId&gt;org.testng&lt;/groupId&gt; &lt;artifactId&gt;testng&lt;/artifactId&gt; &lt;version&gt;6.8&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 运行示例TestNg的对象注入需要继承AbstractTestNGSpringContextTests类。四个元注解 @Target,@Retention,@Documented,@Inherited，是专门用来定义注解的注解 @Target 表示该注解用于什么地方，可能的值在枚举类 ElemenetType 中，包括： ElemenetType.CONSTRUCTOR———–构造器声明 ElemenetType.FIELD ———————–域声明（包括 enum 实例） ElemenetType.LOCAL_VARIABLE——- 局部变量声明 ElemenetType.METHOD ——————方法声明 ElemenetType.PACKAGE ——————————— 包声明 ElemenetType.PARAMETER ——————————参数声明 ElemenetType.TYPE———————— 类，接口（包括注解类型）或enum声明@Retention 表示在什么级别保存该注解信息。可选的参数值在枚举类型 RetentionPolicy 中： RetentionPolicy.SOURCE ——————注解将被编译器丢弃 RetentionPolicy.CLASS ———– ———注解在class文件中可用，但会被VM丢弃 RetentionPolicy.RUNTIME ——-将在运行期也保留注释，因此可以通过反射机制读取注解的信息。@Documented 将此注解包含在 javadoc 中 ，它代表着此注解会被javadoc工具提取成文档。在doc文档中的内容会因为此注解的信息内容不同而不同。相当与@see,@param 等。@Inherited 允许子类继承父类中的注解。 TestNG的常用注解@BeforeSuite@AfterSuite@BeforeTest@AfterTest@BeforeClass@AfterClass@BeforeMethod@AfterMethod@Test alwaysRun控制是否每次都执行，dependsOnMethods是一种依赖，依赖某个方法执行，dataProvider可以指定测试数据。自定义注解 实现ITestListener接口ITestListener接口定义调用方式，加注解，通过@Listeners注解嵌入自己的处理逻辑AbstractTestNGSpringContextTests的实现 对集成了spring TestContext Framework与TestNG环境中的ApplicationContext测试支持的基础测试类进行了抽象。当你继承AbstractTestNGSpringContextTests时，就可以访问到下列protected的成员变量：applicationContext：使用它进行显式的bean查找或者测试整个上下文的状态。ApplicationContextAware示例，调用方自身也要被容器实例化，才能拿到实例化的上下文ApplicationContextWebApplicationContext也是对ApplicationContext的一种扩展BeanFactory是根本。DefaultListableBeanFactory是基本实现。XmlBeanFactory是扩展。 资源的定位，加载以及注册。 AbstractTestNGSpringContextTests的初始化以及监听的注册委派模式(Delegate) 委派模式（Delegate）是面向对象设计模式中常用的一种模式。这种模式的原理为类B和类A是两个互相没有任何关系的类，B具有和A一模一样的方法和属性；并且调用B中的方法，属性就是调用A中同名的方法和属性。B好像就是一个受A授权委托的中介。第三方的代码不需要知道A的存在，也不需要和A发生直接的联系，通过B就可以直接使用A的功能，这样既能够使用到A的各种公能，又能够很好的将A保护起来了。一举两得，岂不很好！ 通过DependencyInjectionTestExecutionListener创建测试类对象AbstractTransactionalTestNGSpringContextTests 继承该类的测试用例在spring管理的事务中进行，测试完后对数据库的记录不会造成任何影响。你对数据库进行一些操作后，它会自动把数据库回滚，这样就保证了你的测试对于环境没有任何影响。 IHookableIHookable 监听器提供了类似与面向方面编程（AOP）中的Around Advice 的功能。它在测试方法执行前后提供了切入点，从而使用户能够在测试方法运行前后注入特定的功能。例如，用户可以在当前测试方法运行前加入特定的验证逻辑以决定测试方法是否运行或者跳过，甚至覆盖测试方法的逻辑。下面是 IHookable 监听器要求实现的方法签名。1void run(IHookCallBack callBack, ITestResult testResult) 如要运行原始测试方法逻辑，需要调用 runTestMethod 方法。1callBack.runTestMethod(testResult); TestNg多线程示例支持方法依赖调用","categories":[],"tags":[]}]}